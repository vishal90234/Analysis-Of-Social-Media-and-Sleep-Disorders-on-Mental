{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "How accurately can machine learning models predict mental health outcomes based on social media usage patterns and demographic factors?"
      ],
      "metadata": {
        "id": "B6S44Bu9FAPC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "IlHqnXYEEmJl",
        "outputId": "5ac7601a-227e-47bf-a0eb-56bf28f6d0b5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8672b105-2416-45d4-8e01-45dd90ace88d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8672b105-2416-45d4-8e01-45dd90ace88d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving mental_health_and_technology_usage_2022.xlsx to mental_health_and_technology_usage_2022.xlsx\n",
            "User uploaded file \"mental_health_and_technology_usage_2022.xlsx\" with length 965160 bytes\n"
          ]
        }
      ],
      "source": [
        "# prompt: upload excl file from gogle colan and print\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace the file path with the actual path of your Excel file\n",
        "file_path = 'mental_health_and_technology_usage_2022.xlsx'\n",
        "\n",
        "# Load the Excel file into a Pandas DataFrame\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E06HFEBTFEyc",
        "outputId": "f81a3506-7151-4639-8636-d88f7c19316a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the dataset:\n",
            "    Timestap     User_ID  Age  Birth Year Generation  Technology_Usage_Hours  \\\n",
            "0 2022-04-01  USER-00001   23        1999      Gen Z                    6.57   \n",
            "1 2022-04-01  USER-00002   21        2001      Gen Z                    3.01   \n",
            "2 2022-04-01  USER-00003   51        1971      Gen X                    3.04   \n",
            "3 2022-04-01  USER-00004   25        1997      Gen Z                    3.84   \n",
            "4 2022-04-01  USER-00005   53        1969      Gen X                    1.20   \n",
            "\n",
            "   Social_Media_Usage_Hours  Gaming_Hours  Screen_Time_Hours  \\\n",
            "0                      6.00          0.68              12.36   \n",
            "1                      2.57          3.74               7.61   \n",
            "2                      6.14          1.26               3.16   \n",
            "3                      4.48          2.59              13.08   \n",
            "4                      0.56          0.29              12.63   \n",
            "\n",
            "  Mental_Health_Status Stress_Level  Sleep_Hours  Physical_Activity_Hours  \\\n",
            "0                 Good          Low         8.01                     6.71   \n",
            "1                 Poor         High         7.28                     5.88   \n",
            "2                 Fair         High         8.04                     9.81   \n",
            "3            Excellent       Medium         5.62                     5.28   \n",
            "4                 Good          Low         5.55                     4.00   \n",
            "\n",
            "  Support_Systems_Access Work_Environment_Impact Online_Support_Usage  \n",
            "0                     No                Negative                  Yes  \n",
            "1                    Yes                Positive                   No  \n",
            "2                     No                Negative                   No  \n",
            "3                    Yes                Negative                  Yes  \n",
            "4                     No                Positive                  Yes  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs8jOh6HVWKF",
        "outputId": "9c6907f1-99cc-4147-f44d-966fb5526dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['level_0', 'index', 'Timestap', 'User_ID', 'Age', 'Birth Year',\n",
              "       'Generation', 'Technology_Usage_Hours', 'Social_Media_Usage_Hours',\n",
              "       'Gaming_Hours', 'Screen_Time_Hours', 'Mental_Health_Status',\n",
              "       'Stress_Level', 'Sleep_Hours', 'Physical_Activity_Hours',\n",
              "       'Support_Systems_Access', 'Work_Environment_Impact',\n",
              "       'Online_Support_Usage'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Encode categorical columns (e.g., Generation, Mental Health Status)\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "df['Mental_Health_Status'] = label_encoder.fit_transform(df['Mental_Health_Status'])\n",
        "df['Generation'] = label_encoder.fit_transform(df['Generation'])\n",
        "\n",
        "# Select features and target\n",
        "features = ['Technology_Usage_Hours', 'Social_Media_Usage_Hours', 'Gaming_Hours',\n",
        "            'Screen_Time_Hours', 'Sleep_Hours', 'Physical_Activity_Hours', 'Age', 'Generation']\n",
        "target = 'Mental_Health_Status'\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "# Scale the features for LSTM\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape input for LSTM [samples, time steps, features]\n",
        "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "aQ3CDS1kFnkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# LSTM layer\n",
        "model.add(LSTM(50, return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "\n",
        "# Dropout to prevent overfitting\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Fully connected layer with one output (for binary classification)\n",
        "model.add(Dense(1, activation='sigmoid'))  # Use 'softmax' for multiclass classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# For multiclass classification, replace 'binary_crossentropy' with 'categorical_crossentropy'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WIn4QnLF4Xk",
        "outputId": "a4b04e26-d1ce-48e0-d6f3-5e189afae9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZLi3agfF7hx",
        "outputId": "3f29580f-80f3-4a72-fdce-433f3dc57755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.2480 - loss: 0.4430 - val_accuracy: 0.2450 - val_loss: -0.7672\n",
            "Epoch 2/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2523 - loss: -1.2684 - val_accuracy: 0.2450 - val_loss: -3.1345\n",
            "Epoch 3/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2442 - loss: -3.6863 - val_accuracy: 0.2450 - val_loss: -6.2010\n",
            "Epoch 4/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2452 - loss: -6.9134 - val_accuracy: 0.2450 - val_loss: -9.8138\n",
            "Epoch 5/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2524 - loss: -10.4796 - val_accuracy: 0.2450 - val_loss: -13.9429\n",
            "Epoch 6/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2465 - loss: -14.8679 - val_accuracy: 0.2450 - val_loss: -18.8514\n",
            "Epoch 7/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2547 - loss: -20.0572 - val_accuracy: 0.2450 - val_loss: -24.4613\n",
            "Epoch 8/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2587 - loss: -25.9312 - val_accuracy: 0.2450 - val_loss: -30.7899\n",
            "Epoch 9/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2509 - loss: -31.3494 - val_accuracy: 0.2450 - val_loss: -37.2122\n",
            "Epoch 10/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2501 - loss: -38.0416 - val_accuracy: 0.2450 - val_loss: -43.1484\n",
            "Epoch 11/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2516 - loss: -41.6221 - val_accuracy: 0.2450 - val_loss: -48.5851\n",
            "Epoch 12/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2517 - loss: -46.9211 - val_accuracy: 0.2450 - val_loss: -53.7259\n",
            "Epoch 13/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2527 - loss: -50.6975 - val_accuracy: 0.2450 - val_loss: -58.5311\n",
            "Epoch 14/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2453 - loss: -56.6811 - val_accuracy: 0.2450 - val_loss: -63.2108\n",
            "Epoch 15/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2393 - loss: -65.9260 - val_accuracy: 0.2450 - val_loss: -67.7141\n",
            "Epoch 16/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2491 - loss: -63.8484 - val_accuracy: 0.2450 - val_loss: -72.1178\n",
            "Epoch 17/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2494 - loss: -67.1086 - val_accuracy: 0.2450 - val_loss: -76.4234\n",
            "Epoch 18/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2470 - loss: -73.5372 - val_accuracy: 0.2450 - val_loss: -80.6813\n",
            "Epoch 19/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2457 - loss: -78.4938 - val_accuracy: 0.2450 - val_loss: -84.8455\n",
            "Epoch 20/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2531 - loss: -82.2724 - val_accuracy: 0.2450 - val_loss: -88.9263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test data\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppmunhZFGD-X",
        "outputId": "79695522-8a44-4cc6-961e-54637b86d6cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2424 - loss: -92.2240\n",
            "Test Accuracy: 0.24899999797344208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on new data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred_class = (y_pred > 0.5).astype(int)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onAYhfmbGI9l",
        "outputId": "7b132205-bc41-4cb4-d887-890c8e2f819f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "20LRciopInzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "df['Mental_Health_Status'] = label_encoder.fit_transform(df['Mental_Health_Status'])\n",
        "df['Generation'] = label_encoder.fit_transform(df['Generation'])\n",
        "\n",
        "features = ['Technology_Usage_Hours', 'Social_Media_Usage_Hours', 'Gaming_Hours',\n",
        "            'Screen_Time_Hours', 'Sleep_Hours', 'Physical_Activity_Hours', 'Age', 'Generation']\n",
        "target = 'Mental_Health_Status'\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "# Step 3: Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape the input to [samples, timesteps, features]\n",
        "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
        "\n",
        "# Step 4: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Handle Class Imbalance (if needed)\n",
        "# Compute class weights to handle imbalanced datasets\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "\n",
        "# Convert class_weights to a dictionary\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "# This maps class indices (0, 1, 2, etc.) to their corresponding weights.\n",
        "\n",
        "\n",
        "# Step 6: Define the LSTM Model\n",
        "model = Sequential()\n",
        "\n",
        "# Add LSTM layers\n",
        "model.add(LSTM(100, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(LSTM(50))\n",
        "\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Output layer for binary classification (change to softmax for multi-class classification)\n",
        "model.add(Dense(1, activation='sigmoid'))  # Use softmax if predicting multiple classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 7: Model Training\n",
        "# Fit the model, using class weights if there's imbalance\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, class_weight=class_weight_dict, validation_split=0.2)\n",
        "# Use class_weight_dict here\n",
        "\n",
        "# Step 8: Model Evaluation\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "# Step 9: Making Predictions\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert predicted probabilities to binary class labels\n",
        "y_pred_class = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Print the first 5 predictions and their corresponding actual values\n",
        "print(\"Predicted Classes: \", y_pred_class[:5].ravel())\n",
        "print(\"Actual Classes: \", y_test[:5])"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p4iNhXyHmvm",
        "outputId": "1cac4abb-e921-44c1-dbc1-7461f226d4ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.2495 - loss: 0.6730 - val_accuracy: 0.2450 - val_loss: 0.6012\n",
            "Epoch 2/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.2386 - loss: 0.5613 - val_accuracy: 0.2450 - val_loss: 0.3869\n",
            "Epoch 3/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2397 - loss: 0.2825 - val_accuracy: 0.2450 - val_loss: -0.1848\n",
            "Epoch 4/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2501 - loss: -0.3909 - val_accuracy: 0.2450 - val_loss: -1.2023\n",
            "Epoch 5/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2408 - loss: -1.4840 - val_accuracy: 0.2450 - val_loss: -2.4711\n",
            "Epoch 6/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2372 - loss: -2.9070 - val_accuracy: 0.2450 - val_loss: -3.9024\n",
            "Epoch 7/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2453 - loss: -4.3107 - val_accuracy: 0.2450 - val_loss: -5.2760\n",
            "Epoch 8/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2464 - loss: -5.2151 - val_accuracy: 0.2450 - val_loss: -6.4570\n",
            "Epoch 9/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2459 - loss: -6.6576 - val_accuracy: 0.2450 - val_loss: -7.4560\n",
            "Epoch 10/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2602 - loss: -7.2293 - val_accuracy: 0.2450 - val_loss: -8.2727\n",
            "Epoch 11/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.2496 - loss: -8.0249 - val_accuracy: 0.2450 - val_loss: -8.9925\n",
            "Epoch 12/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2480 - loss: -8.5994 - val_accuracy: 0.2450 - val_loss: -9.6185\n",
            "Epoch 13/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2499 - loss: -9.8081 - val_accuracy: 0.2450 - val_loss: -10.1848\n",
            "Epoch 14/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2531 - loss: -9.3492 - val_accuracy: 0.2450 - val_loss: -10.7133\n",
            "Epoch 15/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2544 - loss: -11.2548 - val_accuracy: 0.2450 - val_loss: -11.2018\n",
            "Epoch 16/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2499 - loss: -10.6837 - val_accuracy: 0.2450 - val_loss: -11.6629\n",
            "Epoch 17/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2500 - loss: -11.5096 - val_accuracy: 0.2450 - val_loss: -12.1099\n",
            "Epoch 18/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2396 - loss: -12.0591 - val_accuracy: 0.2450 - val_loss: -12.5388\n",
            "Epoch 19/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2567 - loss: -12.0162 - val_accuracy: 0.2450 - val_loss: -12.9579\n",
            "Epoch 20/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2502 - loss: -11.6326 - val_accuracy: 0.2450 - val_loss: -13.3705\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2424 - loss: -13.8698\n",
            "Test Accuracy: 0.24899999797344208\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
            "Predicted Classes:  [1 1 1 1 1]\n",
            "Actual Classes:  [3 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred_class = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Print the predicted values (class labels)\n",
        "print(\"Predicted Class Labels: \", y_pred_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB6GDt2dP5Ul",
        "outputId": "ecd9474c-52f0-487d-d263-5c44e0bf332e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Predicted Class Labels:  [1 1 2 ... 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the predicted probabilities\n",
        "print(\"Predicted Probabilities: \", y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6N8jBN1P8Ze",
        "outputId": "ffe0461f-bf92-47b3-8d12-c1b110732506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Probabilities:  [[0.13675968 0.31579185 0.28575101 0.2616974 ]\n",
            " [0.25943047 0.3071715  0.24181698 0.19158106]\n",
            " [0.2483383  0.19383757 0.3116312  0.24619292]\n",
            " ...\n",
            " [0.25587097 0.2753247  0.23754545 0.23125885]\n",
            " [0.26063883 0.23154579 0.25174993 0.25606543]\n",
            " [0.2891149  0.18150495 0.2672633  0.26211685]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred_class = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Assuming 'Age' is in the features, extract the 'Age' column from the test set\n",
        "age_test = X_test[:, features.index('Age')]\n",
        "\n",
        "# Create age groups by binning the age values into categories\n",
        "age_bins = [0, 18, 30, 40, 50, 60, 100]  # Define the age ranges\n",
        "age_labels = ['<18', '18-30', '31-40', '41-50', '51-60', '60+']  # Labels for each bin\n",
        "age_groups = pd.cut(age_test, bins=age_bins, labels=age_labels)\n",
        "\n",
        "# Combine Age groups and predicted class labels into a DataFrame for easier viewing\n",
        "predictions_with_age_groups = pd.DataFrame({'Age_Group': age_groups, 'Predicted_Stress_Class': y_pred_class})\n",
        "\n",
        "# Group the results by age group and calculate the count of each prediction in each group\n",
        "grouped_predictions = predictions_with_age_groups.groupby('Age_Group')['Predicted_Stress_Class'].value_counts().unstack()\n",
        "\n",
        "# Print the grouped results\n",
        "print(grouped_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NVGuRR6Q7e7",
        "outputId": "509c6c56-0b80-48ff-b989-a52c01f12120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Predicted_Stress_Class    0    1    2    3\n",
            "Age_Group                                 \n",
            "<18                     310  367  102  190\n",
            "18-30                     0    0    0    0\n",
            "31-40                     0    0    0    0\n",
            "41-50                     0    0    0    0\n",
            "51-60                     0    0    0    0\n",
            "60+                       0    0    0    0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-ddc6cfce3c5f>:22: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  grouped_predictions = predictions_with_age_groups.groupby('Age_Group')['Predicted_Stress_Class'].value_counts().unstack()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred_class = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Assuming 'Social_Media_Usage_Hours' is in the features, extract it from the test set\n",
        "social_media_test = X_test[:, features.index('Social_Media_Usage_Hours')]\n",
        "\n",
        "# Create social media usage groups by binning the usage values into categories\n",
        "social_media_bins = [0, 1, 3, 5, 8, 12]  # Define the social media usage ranges (in hours)\n",
        "social_media_labels = ['0-1 hrs', '1-3 hrs', '3-5 hrs', '5-8 hrs', '8+ hrs']  # Labels for each bin\n",
        "social_media_groups = pd.cut(social_media_test, bins=social_media_bins, labels=social_media_labels)\n",
        "\n",
        "# Combine Social Media usage groups and predicted class labels into a DataFrame for easier viewing\n",
        "predictions_with_social_media_groups = pd.DataFrame({'Social_Media_Usage_Group': social_media_groups, 'Predicted_Stress_Class': y_pred_class})\n",
        "\n",
        "# Group the results by social media usage group and calculate the count of each prediction in each group\n",
        "grouped_predictions = predictions_with_social_media_groups.groupby('Social_Media_Usage_Group')['Predicted_Stress_Class'].value_counts().unstack()\n",
        "\n",
        "# Print the grouped results\n",
        "print(grouped_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHHlCCKBRQ4U",
        "outputId": "7826c1af-e207-4da1-ff86-c4fe863ead0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Predicted_Stress_Class      0    1   2    3\n",
            "Social_Media_Usage_Group                   \n",
            "0-1 hrs                   168  167  75  144\n",
            "1-3 hrs                   122  104  78  126\n",
            "3-5 hrs                     0    0   0    0\n",
            "5-8 hrs                     0    0   0    0\n",
            "8+ hrs                      0    0   0    0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-237185ceca91>:22: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  grouped_predictions = predictions_with_social_media_groups.groupby('Social_Media_Usage_Group')['Predicted_Stress_Class'].value_counts().unstack()\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming df is your original DataFrame and the target is 'Mental_Health_Status'\n",
        "features = ['Age', 'Generation', 'Technology_Usage_Hours', 'Social_Media_Usage_Hours',\n",
        "            'Gaming_Hours', 'Screen_Time_Hours', 'Stress_Level', 'Sleep_Hours',\n",
        "            'Physical_Activity_Hours']  # Ensure these match during training and testing\n",
        "\n",
        "# Reset the index of your DataFrame to ensure consistent indexing\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Apply Label Encoding to categorical columns ('Generation' and 'Stress_Level')\n",
        "for column in ['Generation', 'Stress_Level']:  # Add any other categorical features as needed\n",
        "    df[column] = encoder.fit_transform(df[column])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = df[features]\n",
        "y = df['Mental_Health_Status']\n",
        "\n",
        "# Split data while keeping track of the indices\n",
        "X_train, X_test, y_train, y_test, X_train_indices, X_test_indices = train_test_split(\n",
        "    X, y, df.index, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Ensure the shape of X_train and X_test matches\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "\n",
        "# Ensure model is trained on X_train and y_train\n",
        "# Example of model creation (adjust as needed for your specific model)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "# Define a basic model structure (adjust to your needs)\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))  # input_dim = number of features\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification for 'Mental_Health_Status'\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred_class = (y_pred > 0.5).astype(int)  # Assuming binary classification, adjust for multi-class\n",
        "\n",
        "# Extract 'Social_Media_Usage_Hours' and 'Timestamp' from the original df using the test indices\n",
        "social_media_test = df.loc[X_test_indices, 'Social_Media_Usage_Hours'].values\n",
        "time_test = df.loc[X_test_indices, 'Timestap'].values  # Fixing typo from 'Timestap' to 'Timestamp' if needed\n",
        "\n",
        "# Convert 'Timestap' to datetime if necessary\n",
        "time_test = pd.to_datetime(time_test)\n",
        "\n",
        "# Sort by time to analyze predictions over time\n",
        "sorted_indices = np.argsort(time_test)\n",
        "social_media_test_sorted = social_media_test[sorted_indices]\n",
        "y_pred_class_sorted = y_pred_class[sorted_indices]\n",
        "time_test_sorted = time_test[sorted_indices]\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5AruDOVVAlH",
        "outputId": "90970d77-1a42-4b2c-a35f-9760334f39b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (8000, 9)\n",
            "Shape of X_test: (2000, 9)\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2460 - loss: -501.8250\n",
            "Epoch 2/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.2410 - loss: -21115.8516\n",
            "Epoch 3/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.2498 - loss: -121038.8125\n",
            "Epoch 4/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.2519 - loss: -376366.0312\n",
            "Epoch 5/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2536 - loss: -837826.3750\n",
            "Epoch 6/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2422 - loss: -1523003.3750\n",
            "Epoch 7/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2429 - loss: -2556390.0000\n",
            "Epoch 8/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2513 - loss: -3600284.7500\n",
            "Epoch 9/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2511 - loss: -5192044.0000\n",
            "Epoch 10/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2550 - loss: -6791245.0000\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all arrays are 1-dimensional\n",
        "time_test_sorted = np.array(time_test_sorted).ravel()  # Ensures time is 1D\n",
        "social_media_test_sorted = np.array(social_media_test_sorted).ravel()  # Ensures social media usage is 1D\n",
        "y_pred_class_sorted = np.array(y_pred_class_sorted).ravel()  # Ensures predictions are 1D\n",
        "\n",
        "# Check that all arrays have the same length before combining into a DataFrame\n",
        "print(f\"Time length: {len(time_test_sorted)}\")\n",
        "print(f\"Social Media Usage length: {len(social_media_test_sorted)}\")\n",
        "print(f\"Predicted Class length: {len(y_pred_class_sorted)}\")\n",
        "\n",
        "# Combine the time, social media usage, and predicted class labels into a DataFrame\n",
        "predictions_with_time = pd.DataFrame({\n",
        "    'Time': time_test_sorted,\n",
        "    'Social_Media_Usage': social_media_test_sorted,\n",
        "    'Predicted_Stress_Class': y_pred_class_sorted\n",
        "})\n",
        "\n",
        "# Group by time-based intervals (e.g., by month, week) to analyze trends\n",
        "predictions_with_time['Month'] = predictions_with_time['Time'].dt.to_period('M')  # Group by month\n",
        "\n",
        "# Aggregate the data to get the counts of each predicted stress class per time period\n",
        "grouped_predictions_over_time = predictions_with_time.groupby('Month')['Predicted_Stress_Class'].value_counts().unstack()\n",
        "\n",
        "# Print the grouped results\n",
        "print(grouped_predictions_over_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ix-Ip8jWgZk",
        "outputId": "9746605f-3600-4273-94f1-da8d8dbd297d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time length: 2000\n",
            "Social Media Usage length: 2000\n",
            "Predicted Class length: 2000\n",
            "Predicted_Stress_Class    1\n",
            "Month                      \n",
            "2020-07                  65\n",
            "2020-08                 105\n",
            "2022-04                  39\n",
            "2022-05                 202\n",
            "2022-06                 271\n",
            "2022-09                 485\n",
            "2022-10                 299\n",
            "2022-11                 246\n",
            "2022-12                 288\n"
          ]
        }
      ]
    }
  ]
}